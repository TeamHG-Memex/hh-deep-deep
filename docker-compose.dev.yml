version: '2'


services:

  hh-trainer:
    image: hh-deep-deep
    build: .
    command:
      - hh-deep-deep-service
      - trainer
      - --kafka-host=hh-kafka
      - --docker-image=auto-deep-deep-hh  # dev image
      - --host-root=${PWD}
      - --proxy-container=hh-deep-deep-tor-proxy
      - --debug                           # debug

  hh-crawler:
    image: hh-deep-deep
    build: .
    command:
      - hh-deep-deep-service
      - crawler
      - --kafka-host=hh-kafka
      - --docker-image=auto-dd-crawler-hh  # dev image
      - --host-root=${PWD}
      - --proxy-container=hh-deep-deep-tor-proxy
      - --debug                            # debug
      - --max-workers=2                    # limit to only 2 workers

  hh-deepcrawler:
    image: hh-deep-deep
    build: .
    command:
      - hh-deep-deep-service
      - deepcrawler
      - --kafka-host=hh-kafka
      - --docker-image=auto-dd-crawler-hh  # dev image
      - --host-root=${PWD}
      - --proxy-container=hh-deep-deep-tor-proxy
      - --debug                            # debug
      - --max-workers=2                    # limit to only 2 workers

  hh-modeler:
    image: hh-page-clf
    command:
      - hh-page-clf-service
      - --kafka-host=hh-kafka
      - --random-pages=/models/random-pages.jl.gz
      - --lda=/models/lda.pkl
      - --debug  # debug

  # Next two are just to build images with up to date hh-page-classifier

  deep-deep:
    image: auto-deep-deep-hh
    build:
      context: ./docker/
      dockerfile: deep-deep.docker
    command:
      - echo
      - "image for deep-deep built"

  dd-crawler:
    image: auto-dd-crawler-hh
    build:
      context: ./docker/
      dockerfile: dd-crawler.docker
    entrypoint:
      - echo
      - "image for dd-crawler built"
