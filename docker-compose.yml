version: '2'


services:

  hh-trainer:
    image: docker.hyperiongray.com/hh-deep-deep:0.1.2  # update below too!
    restart: always
    network_mode: host
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./deep-deep-jobs:/opt/hh-deep-deep/deep-deep-jobs
    command:
      - hh-deep-deep-service
      - trainer
      - --kafka-host=hh-kafka
      - --docker-image=auto-deep-deep-hh
      - --host-root=${PWD}
      - --debug

  hh-crawler:
    image: docker.hyperiongray.com/hh-deep-deep:0.1.2
    restart: always
    network_mode: host
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./dd-jobs:/opt/hh-deep-deep/dd-jobs
    command:
      - hh-deep-deep-service
      - crawler
      - --kafka-host=hh-kafka
      - --docker-image=auto-dd-crawler-hh
      - --host-root=${PWD}
      - --debug

  hh-modeler:
    image: docker.hyperiongray.com/hh-page-clf:0.2.5  # update git submodule too
    restart: always
    network_mode: host
    volumes:
      - ./models:/models
    command:
      - hh-page-clf-service
      - --kafka-host=hh-kafka
      - --random-pages=/models/random-pages.jl.gz
      - --lda=/models/lda.pkl
      - --debug

  # Next two are just to build images with up to date hh-page-classifier

  deep-deep:
    image: auto-deep-deep-hh
    build:
      context: ./docker/
      dockerfile: deep-deep.docker
    command:
      - echo
      - "image for deep-deep built"

  dd-crawler:
    image: auto-dd-crawler-hh
    build:
      context: ./docker/
      dockerfile: dd-crawler.docker
    entrypoint:
      - echo
      - "image for dd-crawler built"
